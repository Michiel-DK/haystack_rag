{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import gradio as gr\n",
    "from typing import List\n",
    "from haystack import component, Pipeline, Document\n",
    "from haystack.components.builders import PromptBuilder\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.generators.chat.openai import OpenAIChatGenerator\n",
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.joiners import BranchJoiner\n",
    "from haystack_experimental.components.tools import OpenAIFunctionCaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack_integrations.components.generators.ollama import OllamaGenerator, OllamaChatGenerator\n",
    "\n",
    "llm = OllamaGenerator(model=\"llama3.1:latest\",\n",
    "                            url = \"http://localhost:11434\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "Answer the questions based on the given context.\n",
    "\n",
    "Context:\n",
    "{% for document in documents %}\n",
    "    {{ document.content }}\n",
    "{% endfor %}\n",
    "Question: {{ question }}\n",
    "Answer:\n",
    "\"\"\"\n",
    "rag_pipe = Pipeline()\n",
    "rag_pipe.add_component(\"prompt_builder\", PromptBuilder(template=template))\n",
    "rag_pipe.add_component(\"llm\", llm)\n",
    "\n",
    "rag_pipe.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_pipeline_func(query: str):\n",
    "    documents = [\n",
    "        Document(content=\"My name is Jean and I live in Paris.\"),\n",
    "        Document(content=\"My name is Mark and I live in Berlin.\"),\n",
    "        Document(content=\"My name is Giorgio and I live in Rome.\"),\n",
    "        Document(content=\"My name is Marta and I live in Madrid.\"),\n",
    "        Document(content=\"My name is Harry and I live in London.\"),\n",
    "    ]\n",
    "    result = rag_pipe.run({\"prompt_builder\": {\"question\": query, \n",
    "                                              \"documents\": documents}})\n",
    "    return {\"reply\": result[\"llm\"][\"replies\"][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEATHER_INFO = {\n",
    "    \"Berlin\": {\"weather\": \"mostly sunny\", \"temperature\": 7, \"unit\": \"celsius\"},\n",
    "    \"Paris\": {\"weather\": \"mostly cloudy\", \"temperature\": 8, \"unit\": \"celsius\"},\n",
    "    \"Rome\": {\"weather\": \"sunny\", \"temperature\": 14, \"unit\": \"celsius\"},\n",
    "    \"Madrid\": {\"weather\": \"sunny\", \"temperature\": 10, \"unit\": \"celsius\"},\n",
    "    \"London\": {\"weather\": \"cloudy\", \"temperature\": 9, \"unit\": \"celsius\"},\n",
    "}\n",
    "\n",
    "def get_current_weather(location: str):\n",
    "    if location in WEATHER_INFO:\n",
    "        return WEATHER_INFO[location]\n",
    "    else:\n",
    "        return {\"weather\": \"sunny\", \"temperature\": 70, \"unit\": \"fahrenheit\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"rag_pipeline_func\",\n",
    "            \"description\": \"Get information about where people live\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\",\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"query\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"The city\"}\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_generator = OllamaChatGenerator(model=\"llama3.1:latest\",\n",
    "                            url = \"http://localhost:11434\", generation_kwargs={'tools': tools})\n",
    "replies = chat_generator.run(messages=[ChatMessage.from_user(\"Where does Mark live?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function_caller = OpenAIFunctionCaller(available_functions={\"rag_pipeline_func\": rag_pipeline_func, \n",
    "                                                            \"get_current_weather\": get_current_weather})\n",
    "\n",
    "results = function_caller.run(messages=replies['replies'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(results[\"function_replies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_collector = BranchJoiner(List[ChatMessage])\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\", generation_kwargs={'tools': tools})\n",
    "function_caller = OpenAIFunctionCaller(available_functions={\"rag_pipeline_func\": rag_pipeline_func, \n",
    "                                                            \"get_current_weather\": get_current_weather})\n",
    "\n",
    "chat_agent = Pipeline()\n",
    "chat_agent.add_component(\"message_collector\", message_collector)\n",
    "chat_agent.add_component(\"generator\", chat_generator)\n",
    "chat_agent.add_component(\"function_caller\", function_caller)\n",
    "\n",
    "chat_agent.connect(\"message_collector\", \"generator.messages\")\n",
    "chat_agent.connect(\"generator\", \"function_caller\")\n",
    "chat_agent.connect(\"function_caller.function_replies\", \"message_collector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"\"\"If needed, break down the user's question into simpler questions and follow-up questions that you can use with your tools.\n",
    "        Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\"\"\n",
    "    )\n",
    "]\n",
    "while True:\n",
    "    user_input = input(\"INFO: Type 'exit' or 'quit' to stop\\n\")\n",
    "    if user_input.lower() == \"exit\" or user_input.lower() == \"quit\":\n",
    "        break\n",
    "    messages.append(ChatMessage.from_user(user_input))\n",
    "    response = chat_agent.run({\"message_collector\": {\"value\": messages}})\n",
    "    messages.extend(response['function_caller']['assistant_replies'])\n",
    "    print(response['function_caller']['assistant_replies'][0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"Can you tell me where Giorgio lives?\",\n",
    "        \"What's the weather like in Madrid?\",\n",
    "        \"Who lives in London?\",\n",
    "        \"What's the weather like where Mark lives?\",\n",
    "    ],\n",
    "    title=\"Ask me about weather or where people live!\",\n",
    ")\n",
    "demo.launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "haystack_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
